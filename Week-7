
!pip install requests
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2022.9.24)
Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)
Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)
import requests
url = "https://raw.githubusercontent.com/NVDLI/LDL/main/pt_framework/utilities.py" 
r=requests.get(url) 
with open('utilities.py', 'w') as f: 
  f.write(r.text)
import torch
import torch.nn as nn
import torchvision
import torchvision. transforms as transforms
from torchvision.datasets import CIFAR10 
from torch.utils.data import DataLoader
import numpy as np
from utilities import train_model

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

EPOCHS = 30

BATCH_SIZE=32



transform = transforms.Compose([transforms. ToTensor()]) 
trainset=CIFAR10(root="./pt_data", train=True, download=True, transform=transform) 
trainloader=DataLoader(trainset, batch_size=len(trainset), shuffle=False) 
data=next(iter (trainloader))

mean = data[0].mean()

stddev = data[0].std()
Files already downloaded and verified
We imported the data and imported all the needed liberies for the data. We gave epoch size and batch size. We downloaded the data from the torch libary wth the help of dataloader.

transform=transforms.Compose( 
    [transforms.ToTensor(),
     transforms.Normalize(mean,stddev)])

trainset = CIFAR10(root='./pt_data', train=True, download=True,transform=transform) 
testset = CIFAR10(root='./pt_data', train=False, download=True,transform=transform)


model = nn. Sequential(
    nn.Conv2d(3, 64, 5, stride=2, padding=2),
    nn.ReLU(),
    nn. Conv2d(64, 64, 3, stride=2, padding=1),
    nn.ReLU(),
    nn.Flatten(),
    nn.Linear (64*8*8, 10)
)
Files already downloaded and verified
Files already downloaded and verified
Here we defined what are the test and train data. We got CIFAR10. Then we added Convolutional layers, 3 by 3 size karnel, with 2 padding. So behind it, there is a multiplication with 3 b3 size matrix. Then activation was ReLu. Then we flattened all as a list to male a model.

layers = next (model.modules())
conv_layer0=layers[0]

conv_layer1=layers[2]

output_layer=layers[5]


nn.init.kaiming_normal_(conv_layer0.weight)

nn.init.constant(conv_layer0.bias,0.0)

nn.init.kaiming_normal_(conv_layer1.weight)

nn.init.constant (conv_layer1.bias, 0.0)

nn. init.xavier_uniform (output_layer.weight)

nn.init.constant (output_layer.bias, 0.0)

optimizer = torch.optim.Adam(model.parameters()) 
loss_function = nn.CrossEntropyLoss()

train_model(model, device, EPOCHS, BATCH_SIZE, trainset, testset, optimizer, loss_function, 'acc')
/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  # This is added back by InteractiveShellApp.init_path()
/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  from ipykernel import kernelapp as app
/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
Epoch 1/30 loss: 1.4115 - acc: 0.5003 - val_loss: 1.2192 - val_acc: 0.5659
Epoch 2/30 loss: 1.0736 - acc: 0.6258 - val_loss: 1.0885 - val_acc: 0.6196
Epoch 3/30 loss: 0.9316 - acc: 0.6753 - val_loss: 1.0566 - val_acc: 0.6343
Epoch 4/30 loss: 0.8332 - acc: 0.7091 - val_loss: 1.0670 - val_acc: 0.6392
Epoch 5/30 loss: 0.7520 - acc: 0.7395 - val_loss: 1.0460 - val_acc: 0.6474
Epoch 6/30 loss: 0.6828 - acc: 0.7614 - val_loss: 1.0842 - val_acc: 0.6460
Epoch 7/30 loss: 0.6208 - acc: 0.7822 - val_loss: 1.1176 - val_acc: 0.6424
Epoch 8/30 loss: 0.5665 - acc: 0.7992 - val_loss: 1.1783 - val_acc: 0.6422
Epoch 9/30 loss: 0.5204 - acc: 0.8170 - val_loss: 1.2223 - val_acc: 0.6497
Epoch 10/30 loss: 0.4743 - acc: 0.8307 - val_loss: 1.3527 - val_acc: 0.6297
Epoch 11/30 loss: 0.4359 - acc: 0.8448 - val_loss: 1.3891 - val_acc: 0.6364
Epoch 12/30 loss: 0.4030 - acc: 0.8551 - val_loss: 1.4974 - val_acc: 0.6319
Epoch 13/30 loss: 0.3743 - acc: 0.8661 - val_loss: 1.5685 - val_acc: 0.6318
Epoch 14/30 loss: 0.3451 - acc: 0.8763 - val_loss: 1.6695 - val_acc: 0.6291
Epoch 15/30 loss: 0.3205 - acc: 0.8833 - val_loss: 1.7489 - val_acc: 0.6301
Epoch 16/30 loss: 0.2967 - acc: 0.8939 - val_loss: 1.8392 - val_acc: 0.6244
Epoch 17/30 loss: 0.2800 - acc: 0.8971 - val_loss: 2.0382 - val_acc: 0.6208
Epoch 18/30 loss: 0.2634 - acc: 0.9050 - val_loss: 2.0063 - val_acc: 0.6242
Epoch 19/30 loss: 0.2474 - acc: 0.9116 - val_loss: 2.1842 - val_acc: 0.6184
Epoch 20/30 loss: 0.2387 - acc: 0.9139 - val_loss: 2.2794 - val_acc: 0.6164
Epoch 21/30 loss: 0.2195 - acc: 0.9205 - val_loss: 2.4347 - val_acc: 0.6152
Epoch 22/30 loss: 0.2083 - acc: 0.9229 - val_loss: 2.4376 - val_acc: 0.6071
Epoch 23/30 loss: 0.2117 - acc: 0.9226 - val_loss: 2.5441 - val_acc: 0.6169
Epoch 24/30 loss: 0.1944 - acc: 0.9296 - val_loss: 2.7032 - val_acc: 0.6099
Epoch 25/30 loss: 0.1917 - acc: 0.9310 - val_loss: 2.7599 - val_acc: 0.6164
Epoch 26/30 loss: 0.1781 - acc: 0.9360 - val_loss: 2.8841 - val_acc: 0.6126
Epoch 27/30 loss: 0.1772 - acc: 0.9365 - val_loss: 2.9249 - val_acc: 0.6146
Epoch 28/30 loss: 0.1711 - acc: 0.9385 - val_loss: 3.0027 - val_acc: 0.6107
Epoch 29/30 loss: 0.1666 - acc: 0.9406 - val_loss: 3.2499 - val_acc: 0.6079
Epoch 30/30 loss: 0.1639 - acc: 0.9419 - val_loss: 3.3154 - val_acc: 0.6074
[0.9419385796545106, 0.6074281150159745]
The next process is training. In order to train the model we have to give weights. So each weight is added before giving to the train.

